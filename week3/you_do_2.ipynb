{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pain\\AppData\\Local\\Temp\\ipykernel_15052\\2722253350.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',\n",
    "                 names=['user_id', 'item_id', 'rating', 'timestamp']) \n",
    " \n",
    "r = df.pivot(index='user_id', columns='item_id', values='rating').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nan element: 1486126 total not nan element: 100000\n"
     ]
    }
   ],
   "source": [
    "print(f\"total nan element: {np.count_nonzero(np.isnan(r))} total not nan element: {np.count_nonzero(~np.isnan(r))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_indexes(data:np.ndarray,ratio:float=0.20)->np.ndarray:\n",
    "    \"\"\"\n",
    "    data: numpy array\n",
    "    ratio: float, ratio of the data to be splitted\n",
    "    return: indexes of test data\n",
    "     \"\"\"\n",
    "    not_nan_elements_indexes=np.argwhere(~np.isnan(r))\n",
    "    split_number = int(len(not_nan_elements_indexes)*ratio)\n",
    "    idx = np.random.choice(len(not_nan_elements_indexes), split_number, replace=False)\n",
    "    return not_nan_elements_indexes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_maker(data:np.ndarray, indexes:np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    r: numpy array\n",
    "    indexes: indexes of test data\n",
    "    return: train data\n",
    "    \"\"\"\n",
    "    return_data=data.copy()\n",
    "    for index in indexes:\n",
    "        return_data[index[0]][index[1]] = np.nan\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_1_function(raw_data:np.ndarray,train_data:np.ndarray,test_indexes:np.ndarray,alpha:float=0.001,epoch:int=500):\n",
    "  \"\"\"\n",
    "  raw_data: numpy array\n",
    "  train_data: numpy array\n",
    "  test_indexes :numpy array\n",
    "  alpha: float\n",
    "\n",
    "  raw_data for validation total error\n",
    "  train_data for training\n",
    "  test_indexes for test data indexes\n",
    "  alpha is learning rate\n",
    "\n",
    "  \"\"\"\n",
    "  m,n=train_data.shape\n",
    "  #initilize user_b and item_b\n",
    "  user_b= np.random.random(m) \n",
    "  item_b=np.random.random(n)\n",
    "  row,col=np.where(~np.isnan(train_data))\n",
    "  with trange(epoch) as epoch_size:\n",
    "    for _ in epoch_size:\n",
    "      total_error=0\n",
    "      test_error=0\n",
    "      prev_user_b=user_b.copy()\n",
    "      prev_item_b=item_b.copy()\n",
    "      for i,j in zip(row,col):\n",
    "        y_pred=user_b[i]+item_b[j]\n",
    "        e=train_data[i][j]-y_pred\n",
    "        #gradients for user_b and item_b\n",
    "        g_user=-e\n",
    "        g_item=-e\n",
    "        #update user_b and item_b\n",
    "        user_b[i]-=alpha*g_user\n",
    "        item_b[j]-=alpha*g_item\n",
    "        total_error+=e**2\n",
    "        if np.linalg.norm(user_b - prev_user_b) < (alpha / 10) and np.linalg.norm(item_b - prev_item_b) < (alpha / 10):  \n",
    "          print(f\"I do early stoping at iteration {_}\")\n",
    "          break\n",
    "      for test_index in test_indexes:\n",
    "            test_pred=user_b[test_index[0]]+item_b[test_index[1]]\n",
    "            test_error+=raw_data[test_index[0]][test_index[1]]-test_pred\n",
    "      epoch_size.set_description(f'Total Square Error: {total_error:.2f} Validation Square Error: {test_error:.2f}')\n",
    "   \n",
    "   \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_2_function(raw_data:np.ndarray,train_data:np.ndarray,test_indexes:np.ndarray,alpha:float=0.001,lambd:float=0.1,epoch:int=500)->np.ndarray:\n",
    "  \"\"\"\n",
    "  raw_data: numpy array\n",
    "  train_data: numpy array\n",
    "  test_indexes :numpy array\n",
    "  alpha: float\n",
    "  lambd: float\n",
    "\n",
    "  raw_data for validation total error\n",
    "  train_data for training\n",
    "  test_indexes for test data indexes\n",
    "  alpha is learning rate\n",
    "  lambd for regulurazation to avoid overfit\n",
    "\n",
    "  \"\"\"\n",
    "  m,n=train_data.shape\n",
    "  test_error=0\n",
    "  #initilize user_b and item_b\n",
    "  user_b= np.random.random(m) \n",
    "  item_b=np.random.random(n)\n",
    "  row,col=np.where(~np.isnan(train_data))\n",
    "  with trange(epoch) as epoch_size:\n",
    "    for _ in epoch_size:\n",
    "      total_error=0\n",
    "      test_error=0\n",
    "      prev_user_b=user_b.copy()\n",
    "      prev_item_b=item_b.copy()\n",
    "      for i,j in zip(row,col):\n",
    "        y_pred=user_b[i]+item_b[j]\n",
    "        e=train_data[i][j]-y_pred\n",
    "        #gradients for user_b and item_b\n",
    "        g_user=-e+ lambd* user_b[i]\n",
    "        g_item=-e + lambd *item_b[j]\n",
    "        #update user_b and item_b\n",
    "        user_b[i]-=alpha*g_user\n",
    "        item_b[j]-=alpha*g_item\n",
    "        total_error+=e**2\n",
    "        if np.linalg.norm(user_b - prev_user_b) < (alpha / 10) and np.linalg.norm(item_b - prev_item_b) < (alpha / 10):  \n",
    "          print(f\"I do early stoping at iteration {_}\")\n",
    "          break\n",
    "      for test_index in test_indexes:\n",
    "            test_pred=user_b[test_index[0]]+item_b[test_index[1]]\n",
    "            test_error+=raw_data[test_index[0]][test_index[1]]-test_pred\n",
    "      epoch_size.set_description(f'Total Square Error: {total_error:.2f} Validation Square Error: {test_error:.2f}')\n",
    "  return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes=find_text_indexes(r)\n",
    "train_data=train_data_maker(r,test_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mpart_1_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mraw_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtest_indexes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepoch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "raw_data: numpy array\n",
      "train_data: numpy array\n",
      "test_indexes :numpy array\n",
      "alpha: float\n",
      "\n",
      "raw_data for validation total error\n",
      "train_data for training\n",
      "test_indexes for test data indexes\n",
      "alpha is learning rate\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\pain\\appdata\\local\\temp\\ipykernel_7884\\3522702363.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?part_1_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Square Error: 41609.90 Validation Square Error: 94.15: 100%|██████████| 500/500 [04:47<00:00,  1.74it/s]  \n"
     ]
    }
   ],
   "source": [
    "part_1_function(raw_data=r,train_data=train_data,test_indexes=test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Square Error: 43300.77 Validation Square Error: 2246.16: 100%|██████████| 500/500 [05:06<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "val_error=part_2_function(raw_data=r,train_data=train_data,test_indexes=test_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for best lambda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_errors=[]\n",
    "lambdas=[0.1,0.2,0.4,0.5]\n",
    "for lambd in lambdas:\n",
    "    val_error=part_2_function(raw_data=r,train_data=train_data,test_indexes=test_indexes,lambd=lambd)\n",
    "    print(val_error)\n",
    "    val_errors.append(val_error)\n",
    "print(f\"Best lambda value is : {lambdas[val_errors.index(min(val_errors))}\")]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f38135ad8333328a84be54b3e68137596243cced89bc8f52440eaf848b0526"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
